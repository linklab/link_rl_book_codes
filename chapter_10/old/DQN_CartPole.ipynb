{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-03-09 15:41:06,717] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (4,)\n",
      "Number of actions: 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "print('State shape: {}'.format(env.observation_space.shape))\n",
    "print('Number of actions: {}'.format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "        \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        #fc1 8 -> 64\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        \n",
    "        #relu1\n",
    "        self.relu1 = nn.PReLU()\n",
    "        \n",
    "        #fc2 64 -> 64\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        \n",
    "        #relu2\n",
    "        self.relu2 = nn.PReLU()\n",
    "        \n",
    "        #fc3 64 -> action_size\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        #input state: [bsz, 8]\n",
    "        \n",
    "        #fc1 8 -> 64\n",
    "        x = self.fc1(state)\n",
    "        \n",
    "        #relu1\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        #fc2 64 -> 64\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #relu2\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        #fc3 64 -> action_size\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(100000) # replay buffer size\n",
    "BATCH_SIZE = 64           # minibatch size\n",
    "GAMMA = 0.99              # discount factor\n",
    "LR = 0.0005               # learning rate\n",
    "UPDATE_EVERY = 4          # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # initialize Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters())\n",
    "        \n",
    "        # replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        \n",
    "        # initialize time step\n",
    "        self.t_step = 0\n",
    "    \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "    \n",
    "    def act(self, state, eps=0.):\n",
    "        # single state to state tensor (batch size = 1)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # set eval mode for local QN \n",
    "        self.qnetwork_local.eval()\n",
    "        \n",
    "        # predict state value with local QN\n",
    "        with torch.no_grad(): # no need to save the gradient value\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        \n",
    "        # set the mode of local QN back to train\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # e-greedy action selection\n",
    "        # return greedy action if prob > eps\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "        # return random action if prob <= eps\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \n",
    "        # unpack epxeriences\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # define loss function: MSELoss\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "        \n",
    "        # get max predicted Q values from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # compute Q targets from current states\n",
    "        Q_targets = rewards + gamma * Q_targets_next * (1-dones)\n",
    "        \n",
    "        # get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_function(Q_expected, Q_targets)\n",
    "        \n",
    "        # minimise the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update target network\n",
    "        self.target_update(self.qnetwork_local, self.qnetwork_target)\n",
    "    \n",
    "    def target_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "        \n",
    "        # soft update\n",
    "        # tau = 1e-3\n",
    "        # for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        #     target_param.data.copy_(tau*local_param.data + (1.0 - tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    # initialize ReplayBuffer\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \n",
    "                                                                \"action\", \n",
    "                                                                \"reward\", \n",
    "                                                                \"next_state\", \n",
    "                                                                \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    # add a new experience to memory\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    # randomly sample a batch of experiences from memory\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    # return the size of the internal memory\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Traing the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def dqn(agent, n_episodes=2000, max_t=200, eps_start=1.0, eps_end=0.01, eps_decay=0.995, fname=\"dqn\"):\n",
    "    \n",
    "    output_path = \"outputs/{}\".format(fname)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    scores = [] # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=10) # last 100 scores\n",
    "    eps = eps_start # initialize epsilon\n",
    "    save_score_threshold = 200\n",
    "    \n",
    "    # for every episode..\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        \n",
    "        # reset state\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reset score to 0\n",
    "        score = 0\n",
    "        \n",
    "        # for every time step until max_t\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            # get action based on e-greedy policy\n",
    "            action = agent.act(state, eps)\n",
    "            \n",
    "            # execute the chosen action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # update the network with experience replay\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            # set next_state as the new state\n",
    "            state = next_state\n",
    "            \n",
    "            # add reward to the score\n",
    "            score += reward\n",
    "            \n",
    "            # if the agent has reached the terminal state, break the loop\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # append the episode score to the deque\n",
    "        scores_window.append(score)\n",
    "        \n",
    "        # append the episode score to the list\n",
    "        scores.append(score)\n",
    "        \n",
    "        # decrease episilon\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        \n",
    "        # display metrics\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "        # save model if the latest average score is higher than 200.0\n",
    "        if np.mean(scores_window) >= save_score_threshold:\n",
    "            print('\\nEnvironment solved in {:d} episodes! \\tAverage Score: {:.2f}'.format(i_episode-10, np.mean(scores_window)))\n",
    "            print('Finish the DDQN agent train!')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 15.60\n",
      "Episode 200\tAverage Score: 11.40\n",
      "Episode 300\tAverage Score: 55.90\n",
      "Episode 400\tAverage Score: 54.50\n",
      "Episode 500\tAverage Score: 153.80\n",
      "Episode 522\tAverage Score: 200.00\n",
      "Environment solved in 512 episodes! \tAverage Score: 200.00\n",
      "Finish the DDQN agent train!\n"
     ]
    }
   ],
   "source": [
    "fname = \"dqn\"\n",
    "agent = DQNAgent(state_size=4, action_size=2, seed=8)\n",
    "dqn(agent, fname=fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(100000) # replay buffer siz›e\n",
    "BATCH_SIZE = 64           # minibatch size\n",
    "GAMMA = 0.99              # discount factor\n",
    "LR = 0.0005               # learning rate\n",
    "UPDATE_EVERY = 4          # how often to update the network\n",
    "\n",
    "class DDQNAgent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # initialize Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        \n",
    "        # replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        \n",
    "        # initialize time step\n",
    "        self.t_step = 0\n",
    "    \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "    \n",
    "    def act(self, state, eps=0.):\n",
    "        # single state to state tensor (batch size = 1)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # set eval mode for local QN \n",
    "        self.qnetwork_local.eval()\n",
    "        \n",
    "        # predict state value with local QN\n",
    "        with torch.no_grad(): # no need to save the gradient value\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        \n",
    "        # set the mode of local QN back to train\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # e-greedy action selection\n",
    "        # return greedy action if prob > eps\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "        # return random action if prob <= eps\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \n",
    "        # unpack epxeriences\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # define loss function: MSELoss\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "        \n",
    "        best_actions = self.qnetwork_local(next_states).max(1)[1].unsqueeze(1)\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, best_actions)\n",
    "        \n",
    "        # compute Q targets from current states\n",
    "        Q_targets = rewards + gamma * Q_targets_next * (1-dones)\n",
    "        \n",
    "        # get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_function(Q_expected, Q_targets)\n",
    "        \n",
    "        # minimise the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update target network\n",
    "        self.target_update(self.qnetwork_local, self.qnetwork_target)\n",
    "    \n",
    "    def target_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "            \n",
    "        # soft update\n",
    "#         tau = 1e-3\n",
    "#         for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "#             target_param.data.copy_(tau*local_param.data + (1.0 - tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-01-23 17:25:16,778] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 14.70\n",
      "Episode 200\tAverage Score: 23.10\n",
      "Episode 300\tAverage Score: 55.50\n",
      "Episode 400\tAverage Score: 63.20\n",
      "Episode 500\tAverage Score: 194.90\n",
      "Episode 508\tAverage Score: 200.00\n",
      "Environment solved in 498 episodes! \tAverage Score: 200.00\n",
      "Finish the DDQN agent train!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "agent = DDQNAgent(state_size=4, action_size=2, seed=10)\n",
    "dqn(agent, fname='ddqn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritize Experience Replay + DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(100000) # replay buffer size\n",
    "BATCH_SIZE = 64           # minibatch size\n",
    "GAMMA = 0.99              # discount factor\n",
    "TAU = 1e-3                # for soft update of target parameters\n",
    "LR = 0.0005               # learning rate\n",
    "UPDATE_EVERY = 4          # how often to update the network\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size, batch_size, prob_alpha=0.6, prob_beta=0.5):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.prob_beta = prob_beta\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "        self.priorities = np.zeros((buffer_size, ), dtype=np.float32) # long array\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \n",
    "                                                                \"action\", \n",
    "                                                                \"reward\", \n",
    "                                                                \"next_state\",\n",
    "                                                                \"done\"])\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # if self.buffer is empty return 1.0, else max\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        exp = self.experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        # if buffer has rooms left\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer[self.pos] = exp\n",
    "        \n",
    "        # assign max priority\n",
    "        self.priorities[self.pos] = max_priority\n",
    "        \n",
    "        # update index\n",
    "        self.pos = (self.pos + 1) % self.buffer_size \n",
    "        \n",
    "    def sample(self, completion):\n",
    "        \n",
    "        beta = self.prob_beta + (1-self.prob_beta) * completion\n",
    "        \n",
    "        # if buffer is maxed out..\n",
    "        if len(self.buffer) == self.buffer_size:\n",
    "            # all priorities are the same as self.priorities\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            # all priorities are up to self.pos cuz it's not full yet\n",
    "            priorities = self.priorities[:self.pos]\n",
    "            \n",
    "        # $ P(i) = (p_i^\\alpha) / \\Sigma_k p_k^\\alpha $\n",
    "        probabilities_a = priorities ** self.prob_alpha\n",
    "        sum_probabilties_a = probabilities_a.sum()\n",
    "        P_i = probabilities_a / sum_probabilties_a\n",
    "        \n",
    "        sampled_indices = np.random.choice(len(self.buffer), self.batch_size, p=P_i)\n",
    "        experiences = [self.buffer[idx] for idx in sampled_indices]\n",
    "        \n",
    "        # $ w_i = ( 1/N * 1/P(i) ) ** \\beta $\n",
    "        # $ w_i = ( N * P(i) ** (-1 * \\beta) ) $\n",
    "        N = len(self.buffer)\n",
    "        weights = ( N * P_i[sampled_indices] ) ** (-1 * beta)\n",
    "        \n",
    "        #  For stability reasons, we always normalize weights by 1/ maxi wi so\n",
    "        #  that they only scale the update downwards.\n",
    "        weights = weights / weights.max()\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([exp.state for exp in experiences if exp is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([exp.action for exp in experiences if exp is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([exp.reward for exp in experiences if exp is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([exp.next_state for exp in experiences if exp is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([exp.done for exp in experiences if exp is not None]).astype(np.uint8)).float().to(device)\n",
    "        weights = torch.from_numpy(np.vstack(weights)).float()\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, sampled_indices, weights\n",
    "        \n",
    "    def update_priorities(self, batch_indicies, batch_priorities):\n",
    "        for idx, priority in zip(batch_indicies, batch_priorities):\n",
    "            self.priorities[idx] = priority\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DDQNPERAgent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # initialize Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        \n",
    "        # replay memory\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "        # initialize time step\n",
    "        self.t_step = 0\n",
    "    \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, completion):\n",
    "        \n",
    "        # save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample(completion)\n",
    "                self.learn(experiences, GAMMA)\n",
    "    \n",
    "    def act(self, state, eps=0.):\n",
    "        # single state to state tensor (batch size = 1)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # set eval mode for local QN \n",
    "        self.qnetwork_local.eval()\n",
    "        \n",
    "        # predict state value with local QN\n",
    "        with torch.no_grad(): # no need to save the gradient value\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        \n",
    "        # set the mode of local QN back to train\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # e-greedy action selection\n",
    "        # return greedy action if prob > eps\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "        # return random action if prob <= eps\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    \n",
    "    def learn(self, experiences, gamma, p_eps = 1e-5):\n",
    "        \n",
    "        # unpack epxeriences\n",
    "        states, actions, rewards, next_states, dones, sampled_indicies, weights = experiences\n",
    "        \n",
    "        \n",
    "        best_actions = self.qnetwork_local(next_states).max(1)[1].unsqueeze(1)\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, best_actions)\n",
    "        \n",
    "        # compute Q targets from current states\n",
    "        Q_targets = rewards + gamma * Q_targets_next * (1-dones)\n",
    "        \n",
    "        # get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        TD_Error = Q_targets - Q_expected\n",
    "        new_priorities = TD_Error.abs().detach().numpy() + p_eps\n",
    "        \n",
    "        loss = (TD_Error.pow(2) * weights).mean()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # minimise the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.memory.update_priorities(sampled_indicies, new_priorities)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update target network\n",
    "        self.target_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "    \n",
    "    def target_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def ddqn_with_per(agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, fname=\"dqn\"):\n",
    "    \n",
    "    output_path = \"outputs/{}\".format(fname)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    scores = [] # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=10) # last 100 scores\n",
    "    eps = eps_start # initialize epsilon\n",
    "    save_score_threshold = 200\n",
    "    \n",
    "    # for every episode..\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        \n",
    "        # episode completion\n",
    "        completion = i_episode / n_episodes\n",
    "        \n",
    "        # reset state\n",
    "        state = env.reset()\n",
    "        \n",
    "        # reset score to 0\n",
    "        score = 0\n",
    "        \n",
    "        # for every time step until max_t\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            # get action based on e-greedy policy\n",
    "            action = agent.act(state, eps)\n",
    "            \n",
    "            # execute the chosen action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # update the network with experience replay\n",
    "            agent.step(state, action, reward, next_state, done, completion)\n",
    "            \n",
    "            # set next_state as the new state\n",
    "            state = next_state\n",
    "            \n",
    "            # add reward to the score\n",
    "            score += reward\n",
    "            \n",
    "            # if the agent has reached the terminal state, break the loop\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # append the episode score to the deque\n",
    "        scores_window.append(score)\n",
    "        \n",
    "        # append the episode score to the list\n",
    "        scores.append(score)\n",
    "        \n",
    "        # decrease episilon\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        \n",
    "        # display metrics\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "        # save model if the latest average score is higher than 200.0\n",
    "        if np.mean(scores_window) >= save_score_threshold:\n",
    "            print('\\nEnvironment solved in {:d} episodes! \\tAverage Score: {:.2f}'.format(i_episode-10, np.mean(scores_window)))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-01-15 02:43:41,541] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 15.20\n",
      "Episode 200\tAverage Score: 27.80\n",
      "Episode 300\tAverage Score: 60.70\n",
      "Episode 386\tAverage Score: 200.00\n",
      "Environment solved in 376 episodes! \tAverage Score: 200.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "agent = DDQNPERAgent(state_size=4, action_size=2, seed=0)\n",
    "ddqn_with_per(agent, fname='ddqn_per')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DuelingNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \n",
    "        super(DuelingNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Input 계층\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Hindden 계층\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # V(s)\n",
    "        self.fc3_to_state_value = nn.Linear(64, 1)\n",
    "        \n",
    "        # A(s, a)\n",
    "        self.fc3_to_action_value = nn.Linear(64, self.action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        x = self.fc1(state)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        v_x = self.fc3_to_state_value(x)\n",
    "        \n",
    "        a_x = self.fc3_to_action_value(x)\n",
    "        \n",
    "        # average\n",
    "        average_operator = (1 / self.action_size) * a_x\n",
    "        \n",
    "        x = v_x + ( a_x - average_operator ) \n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNPERAgent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # 메인 Q, 고정된 타깃 네트워크 초기화\n",
    "        self.qnetwork_local = DuelingNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = DuelingNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        \n",
    "        # 리플레이 메모리\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "        self.t_step = 0\n",
    "    \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, completion):\n",
    "        \n",
    "        # 리플레이 메모리에 경험 저장\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "        # 일정 주기마다 학습 수행\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample(completion)\n",
    "                self.learn(experiences, GAMMA)\n",
    "    \n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # 메인 Q 네트워크 Evaluate 모드 활성화\n",
    "        self.qnetwork_local.eval()\n",
    "        \n",
    "        # predict state value with local QN\n",
    "        with torch.no_grad(): # no need to save the gradient value\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        \n",
    "        # set the mode of local QN back to train\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # e-greedy action selection\n",
    "        # return greedy action if prob > eps\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "        # return random action if prob <= eps\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    \n",
    "    def learn(self, experiences, gamma, p_eps = 1e-5):\n",
    "        \n",
    "        # 리플레이 메모리에서 경험 가져오기\n",
    "        states, actions, rewards, next_states, dones, sampled_indicies, weights = experiences\n",
    "        \n",
    "        # 최적 행동을 메인 Q 네트워크에서 찾기\n",
    "        best_actions = self.qnetwork_local(next_states).max(1)[1].unsqueeze(1)\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, best_actions)\n",
    "        \n",
    "        # 현재 상태로 고정된 타깃 Q 값을 계산\n",
    "        Q_targets = rewards + gamma * Q_targets_next * (1-dones)\n",
    "        \n",
    "        # 예측된 Q 값을 계산\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        # PER을 위한 TD 오차 계산\n",
    "        TD_Error = Q_targets - Q_expected\n",
    "        new_priorities = TD_Error.abs().detach().numpy() + p_eps\n",
    "        \n",
    "        loss = (TD_Error.pow(2) * weights).mean()\n",
    "        \n",
    "        # minimise the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.memory.update_priorities(sampled_indicies, new_priorities)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # target network 업데이트\n",
    "        self.target_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "    \n",
    "    def target_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-03-09 15:41:33,013] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 17.10\n",
      "Episode 200\tAverage Score: 172.70\n",
      "Episode 267\tAverage Score: 200.00\n",
      "Environment solved in 257 episodes! \tAverage Score: 200.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "agent = DNPERAgent(state_size=4, action_size=2, seed=0)\n",
    "ddqn_with_per(agent, fname='dn_per')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
